{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Evaluation\n",
    "\n",
    "In this assignment you'll train several models and evaluate how effectively they predict instances of fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Import the data from `fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_fraud = pd.read_csv('../data/fraud_data.csv')\n",
    "#df_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "#    df_fraud = pd.read_csv('../data/fraud_data.csv')\n",
    "    df_fraud = pd.read_csv('./fraud_data.csv')\n",
    "    percent_fraud = df_fraud.Class.value_counts(1)[1]\n",
    "\n",
    "    # alternate method to get class percentages\n",
    "    #fraud = np.bincount(df_fraud.Class)[1] / len(df_fraud.Class)\n",
    "    return percent_fraud\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = df_fraud.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y = df_fraud.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./fraud_data.csv')\n",
    "#df = pd.read_csv('../data/fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "\n",
    "# Precision = TP / (TP + FP)\n",
    "\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98525073746312686, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    # recall = TP / (TP + FN)\n",
    "    # accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "    \n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score#, accuracy_score\n",
    "    \n",
    "    # instead of importing the acc_score module, can just\n",
    "    # the .score method of the Classifier instance.\n",
    "    \n",
    "    clf_dummy = DummyClassifier(strategy = 'most_frequent')\n",
    "    clf_dummy.fit(X_train, y_train)\n",
    "    \n",
    "    y_dummy_pred = clf_dummy.predict(X_test)\n",
    "    \n",
    "    recall = recall_score(y_test, y_dummy_pred)\n",
    "#    accuracy = accuracy_score(y_test, y_dummy_pred)\n",
    "    accuracy = clf_dummy.score(X_test, y_test)\n",
    "    \n",
    "    return (accuracy, recall)\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99078171091445433, 0.375, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    # recall = TP / (TP + FN)\n",
    "    # accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "    # precision = TP / (TP + FP)\n",
    "    \n",
    "    from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    clf_svc = SVC().fit(X_train, y_train)\n",
    "    y_pred = clf_svc.predict(X_test)\n",
    "    \n",
    "#    scores = [accuracy_score, recall_score, precision_score]\n",
    "#    ans = (score(y_test, y_pred) for score in scores)\n",
    "    \n",
    "    accuracy = clf_svc.score(X_test, y_test)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    return (accuracy, recall, precision)\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    # precision_recall_curve = ROC? \n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    clf = SVC(C = 1e9, gamma = 1e-7).fit(X_train, y_train)\n",
    "    y_scores = clf.decision_function(X_test)\n",
    "    y_pred = []\n",
    "    for score in y_scores:\n",
    "        if score > -220:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "#    real_vs_pred = list(zip(y_test[:20], y_scores[:20]))\n",
    "#    for each in first_few:\n",
    "#        print(each)\n",
    "#    precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "#    confusion = confusion_matrix(y_test, y_scores)\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return confusion\n",
    "\n",
    "#confusion_220 = \n",
    "answer_four()\n",
    "#confusion_220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.825, 0.9375)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "        \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "    logit = LogisticRegression().fit(X_train, y_train)\n",
    "    y_scores = logit.decision_function(X_test)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "    fpr_lg, tpr_lg, _ = roc_curve(y_test, y_scores)\n",
    "    \n",
    "#    print(fpr_lg)\n",
    "#    print(tpr_lg)\n",
    "    \n",
    "    recall_75 = recall[precision == .75]\n",
    "    \n",
    "    list_fpr = [each for each in fpr_lg]    \n",
    "    for i in range(len(list_fpr)):\n",
    "        if ((list_fpr[i] > 0.15) and (list_fpr[i] < .17)):\n",
    "            idx_16 = i\n",
    "            break\n",
    "            \n",
    "    tpr_16 = tpr_lg[idx_16]\n",
    "    return (float(recall_75[0]), tpr_16)\n",
    "    \n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grid_values = [{'C':[0.01, 0.1, 1, 10, 100]}, \n",
    "#               {'penalty':['l1', 'l2']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#logit = LogisticRegression()\n",
    "#grid_values = [{'C':[0.01, 0.1, 1, 10, 100], 'penalty':['l1', 'l2']}]\n",
    "#grid_logit_recall = GridSearchCV(logit, param_grid = grid_values, scoring = 'recall')\n",
    "#grid_logit_recall.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667,  0.76086957],\n",
       "       [ 0.80072464,  0.80434783],\n",
       "       [ 0.8115942 ,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid_logit_recall.cv_results_['mean_test_score'].reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66666667,  0.76086957,  0.80072464,  0.80434783,  0.8115942 ,\n",
       "        0.8115942 ,  0.80797101,  0.8115942 ,  0.80797101,  0.8115942 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid_logit_recall.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_df = pd.DataFrame(grid_logit_recall.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'penalty': 'l1'} 0.666666666667\n",
      "{'C': 0.01, 'penalty': 'l2'} 0.760869565217\n",
      "{'C': 0.1, 'penalty': 'l1'} 0.800724637681\n",
      "{'C': 0.1, 'penalty': 'l2'} 0.804347826087\n",
      "{'C': 1, 'penalty': 'l1'} 0.811594202899\n",
      "{'C': 1, 'penalty': 'l2'} 0.811594202899\n",
      "{'C': 10, 'penalty': 'l1'} 0.807971014493\n",
      "{'C': 10, 'penalty': 'l2'} 0.811594202899\n",
      "{'C': 100, 'penalty': 'l1'} 0.807971014493\n",
      "{'C': 100, 'penalty': 'l2'} 0.811594202899\n"
     ]
    }
   ],
   "source": [
    "#for score, params in zip(test_df.mean_test_score, test_df.params):\n",
    "#    print(params, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grid_logit_recall.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667,  0.76086957],\n",
       "       [ 0.80072464,  0.80434783],\n",
       "       [ 0.8115942 ,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    logit = LogisticRegression()\n",
    "    grid_values = [{'C':[0.01, 0.1, 1, 10, 100], 'penalty':['l1', 'l2']}]\n",
    "    grid_logit_recall = GridSearchCV(logit, param_grid = grid_values, scoring = 'recall')\n",
    "    grid_logit_recall.fit(X_train,y_train)\n",
    "    \n",
    "    ans6 = grid_logit_recall.cv_results_['mean_test_score']\n",
    "    \n",
    "    return ans6.reshape(5,2)\n",
    "\n",
    "answer_six()\n",
    "#print(clf_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the following function to help visualize results from the grid search\n",
    "#def GridSearch_Heatmap(scores):\n",
    "#    plt.figure()\n",
    "#    sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])\n",
    "#    plt.yticks(rotation=0);\n",
    "\n",
    "#GridSearch_Heatmap(answer_six())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
